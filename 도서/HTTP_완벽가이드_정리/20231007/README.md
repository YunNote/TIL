# 웹 로봇

---

`웹로봇`은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램을 뜻한다.

이 웹로봇들은 웹사이트에 다른 웹사이트로 하이퍼링크, 콘텐츠를 따라 이동하면서 발견한 데이터를 처리한다.
우리는 이를 흔히 `크롤러`, `스파이더`, `웜`, `봇`이라고 부르 기도 한다.

---

## 웹 로봇의 예

- 주식시장 서버에 매 분 HTTP GET요청을 보내고, 얻은데이터를 활용하여 주가 추이 그래프를 생성하는 주식 그래프 로봇
- 월드 와이드 웹의 규모와 진화에 대한 통겨정보를 수집하는 웹 통계 조사 로봇, 이것들은 웹을 떠돌면서 페이지의 갯수를 세고 각 페이지의 크기, 언어, 미디어를 기록 
- 검색 데이터베이스를 만들기위해 발견한 모든 문서를 수집하는 검색엔진 봇 
- 상품에 대한 가격 정보를 수집하는 가격 비교 로봇


---

## 크롤러와 크롤링


> `웹 크롤러`는 웬페이지 한개를 호출하고 그 다음 그페이지가 가리키는 모든 웹페이지를 가져와, 계속해서 그다음 페이지딜이 가리키는
> 모든 웹페이지들을 가져온다. 재귀적인 방식으로 순회하는 로봇을 웹 크롤러라고 부른다. 
> 
> 이러한 로봇을 크롤러 혹인 스파이더라고 부른다.

인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 웹 크롤러를 사용한다. 해당 문서들은
나중에 처리되어 검색 가능한 데이터베이스로 만들어져 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

---

### 어디에서 시작하는가: '루트 집합'

크롤러를 시작하기전 어디서부터 시작할지 출발지점을 알려줘야 한다. 이때 크롤러가 방문을 시작하는 URL들의
초기 집합을 루트집합이라고 부른다. 

루트집합을 고를때는 모든 링크를 크롤링 할 수 있도록관심있는 웹페이지들의 대부분을 가져 올수 있도록 충분히 다른 장소에서 URL들을 선택해야한다.

루트집합을 통해 페이지를 돌며 검색하던중 몇몇 웹페이지들은 어떤 링크도 없이 오도가도 못하게 거의 고립되어 있는 경우가 생긴다
이런 경우때문에 대규모 크롤러 제품들은 루트 집합에 새 페이지나 알려지지 않은 페이지들을 추가하는 기능들을 제공한다.

---

### 링크 추출과 상대 링크 정상화 

크롤러는 웹을 돌아다니면서 꾸준히 문서를 검색한다음 각 페이지안에 들어있는 URL링크들을 파싱해서 크롤링할 페이지들의
목록에 추가해야한다.

새 링크를 발견하면 보통 급속히 확장되게 된다. 크롤러들은 간단한 HTML 파싱을해서 링크들을 추출하고 상대 링크를
절대링크로 변환할 필요가 있다.

---

### 순환 피하기

크롤링을 할때 가장 주의해야할 점은 루프나, 순환에 빠지지 않도록 매우 조심해야한다.
P249 에서 제공하는 이미지를 보면 서로 링크된 페이지가 순환이 되며 동일한 페이지를 계속 가져오게 되는 순환에 빠진다
따라서 이 문제를 해결하기 위해서는 방문한 링크에 대해서 알 고 있어야 한다.

---

### 루프와 중복

순환은 최소 다음의 세가지 이유로 인해 크롤러에게 해롭다.

1. 같은 페이지들을 반복해서 가져오기때문에 시간을 낭비하게 되며, 이러한 크롤러가 네트워크 대역폭을 다 차지하고 어떠한 페이지도 가져올 수 없게 되어버릴 수 있다.
2. 크롤러가 같은 페이지를 반복해서 가져오게 되면 웹 서버의 부담이 된다. 만약 크롤러의 네트워크 접근 속도가 충분히 빠르다면, 웹사이트에게 부하가 가해져 실제 사용자도 사이트에 접근할 수 없게 막아버리게 될 수 있다. 따라서 이러한 행위가 법적인 문제제기의 근거가 될 수 있다.
3. 수많은 중복페이지를 가져오게 되고, 크롤러의 애플리케이션은 중복된 컨텐츠로 넘쳐나게 된다.

---

### 빵 부스러기의 흔적 

방문한 사이트를 지속적으로 추적하는 것은 쉽지 않다, 지금 순간에도 동적으로 생성된 콘텐츠를 제외하더라도 수십억 개의 
서로 다른 웹페이지들이 존재한다.

대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 기법은 다음과 같다.

#### 트리와 해시 테이블
> 복잡한 로봇이라면 방문한 URl을 추적하기 위해 검색트리나 해시테이블을 사용할 수 있다. 해당 자료구조는 URL을 훨씬 빨리 찾아볼 수 있게 해준다.

#### 느슨한 존재 비트맵
> 공간 사용을 최소화하기 위해 존재비트배열과 같은 느슨한 자료구조를 사용한다. 각 URL은 함수에 의해 고정된 크기의 숫자로 변환되고
> 배열 안에 대응하는 존재비트를 갖는다. 크롤링이 되었을때 해당하는 존재비트가 만들어지고 만약 존재비트가 이미 존재한다면 그 URL을 이미 크롤링 되었다고 간주한다.


#### 체크 포인트
> 방문한 URL의 목록이 디스크에 저장되었는지 확인한다.

#### 파티셔닝 
>한대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능할정도로 웹이 성장하였다. 따라서 여러개의 분리된 크롤링봇으로 
> URL들의 특정한 부분을 할당하여 책임을 진다, 개별 로봇들은 URL들을 넘겨주거나, 오동작하는 동료를 도와주거나
> 그 외의 이유로 활동을 조정하기 위해 커뮤니케이션을 한다.

---

### 별칭과 로봇 순환 

올바른 자료구조를 바탕으로 순환하면서 크롤링 하더라도 URL이 별칭을 가질 수 있는 이상 어떠한 페이지를 이전에
방문했었는지 확인하는것이 쉽지 않을때도 있다. 같은 URL에 대해서 서로 다른 별칭을 가지고 있다면 달라보이더라도 
결국은 서로 같은 리소스를 가리키게 되는것이다. URL정규화하는 방식을 통해 위 문제를 일부분 해결할 수 있다.

---

### URL 정규화하기

대부분의 웹 로봇은 URL들을 표준 형식으로 `정규화`함으로써 서로 다른 URL이지만 서로 같은 리소스를 가리키고 있는 URL에 대해서
제거하려고 시도한다. 정규화는 다음과 같은 방식으로 변환할 수 있다.

1. 포트 번호가 명시되어 있지 않다면 호스트명에 포트를 추가한다.
2. 모든 %xx 이스케이핑 되어있는 문자들에 대해서 대응되는 문자로 변환한다.
3. `#`태그들을 제거한다. 

위와 같은 방법으로 위 세가지에 해당 하는 방식만 해결 할 수 있다.

그렇다면 해결하지 못하는 방법은 뭐가있을까?

- 서버가 대소문자를 구분하지 않는경우
- 기본페이지가 index.html을 직접 가리킬때
- 도메인이 아닌 ip를 통한 접근을 하였을떄

위 세가지 방법에 대해서는 앞서나온 방식으로는 정규화가 불가능할듯하다.

웹서버에 대한 지식없이는 위와 같은 중복을 피할수 있는 좋은 방법은 없다...

---

### 파일 시스템 링크 순환

파일 시스템의 심볼릭 링크는 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉토리 계층을 만들 수 있다.
따라서 관리자가 실수로 만들기도 하지만, 때로는 웹 마스터가 크롤러를 순환 시키기 위해 악의적으로 순환계층을 만들기도 한다.

예를 들면 다음과 같다.
A의 하위 계층에 B와 C가 있지만 해당 C에서 A를 가리키도록 링크되어있다면 무한적으로 서브링크가 붙어 호출되므로
순환되지만 URL이 달라보이기 때문에 URL만으로는 크롤러가 눈치채지 못하게 된다.

---

### 동적 가상 웹 공간

위의 내용처럼 악의적으로 순환시키기 위해 의도저적으로 복잡한 크롤러 루프를 만드는것은 충분히 발생할 수 있는 일이다.

동적 가상 웹공간은 크롤러를 무한한 가상 웹공간 너머에 있는 나라로 여행을 보내버린다...
이때 웹서버는 파일을 하나도 갖고있지 않을 수 있다. 또한 URL과 HTML은 매번 다르게 하여 크롤러가 순환을 감지하기 어렵다는 것이다.

예를 들면 다음과 같다.

1. 크롤러가 /index.html 을 호출 웹서버에서 /index1.html로 가도록 반환.
2. 크롤러가 다시 /index1.html 호출, 웹서버가 /index2.html로 가도록 반환.

위와 같은 방식을 무한적으로 가상공간을 생성하는 것이다.

---

### 루프와 중복 피하기

모든 순환을 완벽하게 피하는 방법은 없다.또한 웹은 크롤러가 문제를 일으킬 가능성으로 가득차있다. 이러한 문제를 해결하고 올바르게 동작하기위해
사용하는 기법들을 소개한다.


#### URL 정규화
> URL을 표준형태로 변경함으로써 같은 리소스를 가리키는 중복된 URL에 대해서 회피한다.

#### 너비 우선 크롤링
> 크롤러들은 크롤링을 할 수 있는 URL들의 집합을 가지고있다.<br>
> 방문할 URL들을 웹사이트 전체에 걸쳐 너비우선으로 스케줄링하면, 순환의 영향을 최소화 할 수 있다<br>
> 만약 깊이우선탐색으로 운영하게 되면 순환을 건드리는 경우 영원히 다른 사이트로 빠져나올 수 없는 순환에 빠지게 된다.

#### 스로틀링
> 크롤러가 웹사이트에서 일정시간동안 가져올 수 있는 페이지수를 제한한다. 크롤러가 지속적으로 해당 사이트에 접근을
> 시도한다면 문제가 될 수 있기때문에 스로틀링을 사용하여 해당 서버에 대한 접근 횟수와 중복 횟수를 제한할 수 있다.

#### URL 크기 제한
> 보통 1KB가 넘는 URL에 대해서 크롤링을 거부할 수 있다. 만약 순환으로 인해서 URL이 계속 길어지게 된다면 결국
> 길이제한으로 인하여 순환이 중단된다. <br>
> 추가로 URL이 점점길어지게되어 순환에 빠지게 되면 웹 마스터가 크롤러를 서비스 거부 공격자로 오해하게 만들수도 있다.<br>
> 
> 다만 주의해야할 점은 크기제한때문에 특정한 컨텐츠들에 대해서는 가져오지 못하는 문제가 발생할 수 있다.<br>
> 많은 사이트들이 URL을 통하여 상태를 관리하기도 한다. 따라서 URL길이가 길어지기도 한다<br>
> 
> 다만 해당 기법은 URL이 특정 크기에 도달할 때마다 로그를 남김으로써, 특정 사이트에서 벌어지는 일에 대해서 모니터링 할 수있어
> 사용자에게는 훌륭한 신호를 제공할 수 있다.
> 

#### URL/사이트 블랙리스트

>순환을 만들어내거나 함정으로 인하여 순환으로 빠트리고자 하는 사이트의 URL목록을 만들어 관리하고, 블랙리스트에 있는 URL들은
> 피하도록 하는 방식이다. 또한 문제를 일으킬만한 URL에 대해서는 블랙리스트에 추가한다.<br>
> 다만 이 방식은 사람의 손을 필요로 한다. 요즘 크롤러들은 이미 악의적이거나 문제를 가지고 있는 URl에 대해서는 블랙리스트를 가지고 있다.


#### 패턴 발견 
> 반복적인 구성요소를 가지고 있는 URL이나 순환과 같은 오설정들의 패턴을보고 잠재적으로 해당 URL들에 대해서 거절한다.

#### 콘텐츠 지문 
> 해당 박식은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산하여 중복된 방분인지에 대해서 확인한 후 크롤링 하지 않도록 한다.
> 
> 체크섬이 같다면 이미 콘텐츠를 방문하였다고 판단한다. 다만 주의해야할 점은 페이지가 서로 다른내용임에도 체크섬이 똑같은 확률이
> 적은 것을 사용해야 한다. 지문 생성용으로는 MD5와 같은 함수가 사용된다.

#### 사람의 모니터링
> 해당 박식은 말그대로 어떠한 기법으로도 해결하지 못할 경우 사용하는 방법이다. 크롤러의 진행 상황을 사용자가
> 모니터링하여 특정한 일이 발생하면 즉각 인지할 수 있도록 로깅을 추가한뒤 문제가 발생하면 사용자가 직접 커스터마이징을 하는것이다


---

## 로봇의 HTTP 

로봇들은 다른 HTTP 클라이언트와 다르지 않다. 로봇 또한 HTTP 명세의 규칙을 지켜야 한다.

### 요청 헤더 식별하기

로봇들이 HTTP를 최소한으로 지원하려고 하더라도 대부분은 신원 식별 헤더(User-Agent HTTP 헤더)를 구현하고 전송한다.
구현한 개발자들은 로봇에 대해 몇가지 헤더를 사이트에 전송하는것이 좋다.

특정 헤더들은 잘못된 크롤러의 소유자를 찾아낼때, 어떤 컨텐츠를 다룰수 있는지에 대한 정보를 전달할 수 있다.

- User-Agent : 서버에게 요청을 만든 로봇(클라이언트)의 이름을 전달한다.
- From : 로봇(클라이언트)의 사용자/관리자의 이메일 주소를 제공한다.
- Accept : 서버에데 어떠한 미디어 타입을 보내도 되는지 말해준다. 이는 관심있는 유형의 컨텐츠만 받게될 것임을 학신하는데 도움된다.
- Referer : 현재 요청 URL을 포함한 문서의 URL을 제공한다.

---

### 가상 호스팅

로봇(클라이언트) 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 많은 현실에서는 요청에 Host 헤더 정보를
포함하지 않는다면 로봇(클라이언트)이 어떤 URL에 대해 잘못된 컨텐츠를 찾게만든다.

따라서 HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.

Host 헤더를 포함하지 않는다면 다음과 같은 문제가 발생할 수 있다. 예를 들면 다음과 같다.

하나의 서버에서 기본적으로 하나의 사이트를 운영하기도 하지만 그렇지 않은 경우 하나의 서버에 두개 이상의 사이트를 운영하기도 한다.

www.sample1.com 과 www.sample2.com을 운영하는 사이트가 있는상태에서 로봇이 www.sample2.com 을 Host헤더 없이 호출한다면
우리는 www.sample2.com 에 대한 정보를 반환한다고 기대하게 된다. 하지만 서버가 기본적으로 www.sample1.com 을 제공하도록
설정되어있다면 Host가 없는 경우 www.sample2.com이 아닌 www.sample1.com 을 반환하여 우리가 원하는 결과를 얻을 수 없게 된다.

---

### 조건부 요청

조건부 요청이랑 특정 시간이나 엔티티 태그를 비교함으로써 로봇(클라이언트)이 받아간 데이터의 마지막 버전 이후
업데이트가 된 내용이 있는지 알아보는 조건부 HTTP 요청을 구현할 수 있다. 

조건부 요청은 HTTP 캐시가 이전에 받아온 리소스의 사본이 유효한지 검사하는 방법과 매우 비슷하다.

---

### 응답 다루기

기본적으로 로봇(클라이언트)들은 데이터를 가져오는것이 주 관심사이기 떄문에 HTTP 메소드의 `GET` 방식을 주로 사용한다.
다만 조건부 요청과 같은 기능을 사용하는 로봇들에 대해서는 상호작용을 위해 여러 종류의 HTTP 응답을 다루기도 한다.

#### 상태코드
> 로봇들은 최소한의 상태코드나 예상할 수 있는 상태코드를 다룰줄 알아야 한다. `200 OK` or `404 NotFound`와 같은 상태코드를 이해할 줄 알아야 한다.
> 명시적으로 이해할 수 없는 상태코드들에 ㅎ대해서는 상태코드가 속한 분류에 근거하야 다뤄야 한다.
> 
> 다만 모든 서버가 항상 적절한 상태코드를 내려주지는 않는다는것도 알아야 한다.

#### 엔티티 
> HTTP 헤더에 임베딩된 정보를  따라 로봇들은 엔티티 자체의 정보를 찾을 수 있다.<br>
> `http-equiv` 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보이다.<br>
> 
> `http-equiv`태그는 콘텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단이기도 하다.

#### User-Agent 타게팅 
> 웹 관리자들은 로봇들이 방문하게 될 수 있다는것을 염두하고 요청을 예상해야 한다.<br>
> User-Agent는 HTTP 요청을 보내는 디바이스와 브라우저 등 사용자 소프트웨어의 식별 정보를 담고 있는 헤더이다.
> 
> 따라서 User-Agent값을 보고 브라우저의 종류에 맞게 컨텐츠를 최적화하기도 한다. 해당 헤더를 잘 활용하면 특정
> 브라우저나 클라이언트에 대응하는 유연한 페이지를 개발할 수 있다.

---

### 부적절하게 동작하는 로봇들

부적절하게 동작하는 몇몇의 로복들은 서버들을 망가트릴 위험이 있다.다음은 몇가지 실수와, 그로 인하여 초래되는 결과들에 대한 내용을 나열한다.

#### 폭주하는 로봇 
> 보통의 로봇들은 일반 사람보다 훨씬 빠르게 HTTP 명세를 만들어 요청을 할 수 있다. 만약 해당 로봇들이
> 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹서버에 극심한 부하를 안겨주게되어 다른 사용자들또한 서비스를 사용하지
> 못하게 만들 수 있다. 따라서 개발자들은 이러한 문제가 발생하지 않도록 보호 장치를 설계하기도 해야한다.

#### 오래된 URL
> 로봇들은 특정 URL을 방문한 후 방문했던 기록을 저장한다. 만약 방문했던 사이트들이 콘텐츠들을 많이 변경하였다면 
> 운이 나쁜경우 로봇들은 사라진 URL에 대해서 접근을 요청하여 서버에 에러로그가 채워져 그 부하로 인해 요청에 대한 수용 능력이
> 떨어지게 되는 불상사를 만든다.

#### 길고 잘못된 URL
> 로봇은 순환이나 오류로 인해 웹사이트에게 크고 의미없는 URl요청을 할 수 있다. 만약 URL이 길다면 서버의 처리능력에 영향을 주고 
> 웹서버를 어지럽게 채우거나, 서버에 장애를 발생시킬 수 있다.

#### 호기심이 지나친 로봇 
> 이는 사용자가 특정 URL을 알리고 싶지 않았지만 로봇이 존재하지도 않는 문서,디렉토리등을 가져오는 방법으로 긁어오게 되면
> 가끔 발생하기도 한다. <br>
> 이는 소유자가 알리고 싶지 않았지만 제거하지 않거나 깜빡한 경우 발생한다.

#### 동적 게이트웨이 접근
> 로봇들은 자신들이 접근하는 것에 대해 언제나 잘알고 있는것은 아니다 만약 로봇들이 게이트웨이를 통해 URL로 요청한다면 이는 특수한 컨텐츠일 수 있고 처리 비용이
> 많이 들 수 있다. 따라서 웹 관리자들은 이러한 순진한 로봇들을 좋아하지는 않는다.

---

## 로봇 차단하기

로봇 커뮤니티에서는 로봇에 의한 웹사이트 접근이 유발할 수 있는 문제를 알고 로봇들을 잘 제어할 수 있는 메커니즘을 제공하는
자발적인 기법을 제안하였다. `Robots Exclusion Standard`라는 이름으로 지어졌지만 
접근을 제어하는 정보를 저장하는 파일의 이름을따 `robots.txt`라고 부른다. 

`robots.txt`는 단순하다. 웹서버는 서버의 루트에 `robots.txt`파일을 제공하며 어떤 로봇이 어떤 부분에
접근할 수 있는지에 대한 정보를 기술한다. 

만약 로봇이 자발적으로 해당 표준을 따른다면 다른 리소스에 접근하기전에 `robots.txt`에 먼저 접근하여 요청하며 특정 리소스를 요청하기전에
권한이 있는지 확인하기 위해 `robots.txt` 를 검사하게 된다.

웹로봇이 `robots.txt`를 확인하고 요청하는 방식은 다음과 같다.

1. 웹로봇이 웹서버에 리소스를 요청을 하기전에 `/robots.txt` 를 요청한다.
2. `/robots.txt`에 작성되어있는 허용된 리소스에 대해서 권한을 확인하고 해당 리소스를 요청한다.
3. 해당 리소스를 반환받는다.

--- 

### 로봇 차단 표준 

로봇 차단 표준은 임시방편으로 마련된 표준이다.  해당 표준이 작성되고있을때는 표준을 소유한 주체가 없었고 부분집합을 제각각 구현하고 있었다. 

해당 표준은 버전의 이름이 잘 정의되어 있지 않지만 세가지 버전이 존재한다. 

- v0.0 : 로봇 배제 표준-Disabllow 지시자를 지원하는 robots.txt 메커니즘
- v1.0 : 웹로봇제어방법-Allow 지시자의 지원 추가
- v2.0 : 로봇 차단을 위한 표준 정규식과 타이밍 정보를 포함, 다만 널리 지원되지는 않는다.

오늘날에는 v0.0이나 v1.0을 표준으로 채택하였으며, v2.0표준은 복잡하여 널리 채택되지 못하였다. 

[로봇 차단 표준 Wiki](https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%B4%87_%EB%B0%B0%EC%A0%9C_%ED%91%9C%EC%A4%80)

### 웹 사이트와 robots.txt 파일들 

웹사이트의 어떤 URL을 방문하기전에 해당 웹 사이트들의 `robots.txt` 파일이 존재한다면 로봇은 해당 파일을 반드시
가져와서 처리해야한다. 

호스트명과 포트번호에 의해 정의되는 웹 사이트가 있다면 사이트 전체에 대한 `robots.txt`파일은 반드시 하나만 있어야 하며 
만약 가상호스팅이 된다면 다른 모든 파일이 그러하듯 가상의 docroot에 서로 다른 `robots.txt`가 존재 할 수 있다.

#### robots.txt 가져오기 

로봇들은 HTTP GET메서드를 활용하여 `robots.txt`리소스를 가져온다. robots.txt가 존재한다면 서버는 해당 파일을 
`text/plain` 본문으로 반환한다. 

만약에 서버가 `/robots.txt` 요청에 대해서 404 NotFound 를 반환한다면 로봇은 해당 서버가 로봇의 접근을 제한하지
않는것으로 간주하여 어떤 파일이든 요청해서 가져가게 된다. 

로봇은 추적가능하도록 From이나 User-Agent헤더를 통해 신원을 넘기고, 사이트 관리자가 로봇에 대해 문의나 불만이 있는경우를 위해
연락처를 제공해야한다. 다음은 크롤러 요청의 예시이다.
```http request
GET /robots.txt HTTP/1.0
Host: www.sample.com
User-Agent: YunJin/2.0
Date: Fri Oct 6 01:46:11 KST 2023
``` 



